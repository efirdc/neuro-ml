{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "from typing import Tuple, Optional, Dict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "import wandb\n",
    "\n",
    "from torch_experiment import TorchExperiment, KeyDataset, TensorDataset\n",
    "from metrics import r2_score\n",
    "from models import DenseNetwork\n",
    "from loss import CosineSimilarityLoss\n",
    "from utils import require_dataset, reconstruct_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create the pytorch datasets\n",
    "\n",
    "with h5py.File('./example_data/nsd.hdf5', 'r') as f:\n",
    "    betas_indices = f['betas_indices'][:]\n",
    "    volume_shape = f['betas_indices'].attrs['volume_shape']\n",
    "    \n",
    "    train_betas = f['train/betas'][:]\n",
    "    train_stimulus = f['train/stimulus'][:]\n",
    "    \n",
    "    test_betas = f['test/betas'][:]\n",
    "    test_stimulus = f['test/stimulus'][:]\n",
    "    \n",
    "train_dataset = KeyDataset({\n",
    "    'betas': TensorDataset(torch.from_numpy(train_betas)),\n",
    "    'stimulus': TensorDataset(torch.from_numpy(train_stimulus))\n",
    "})\n",
    "test_dataset = KeyDataset({\n",
    "    'betas': TensorDataset(torch.from_numpy(test_betas)),\n",
    "    'stimulus': TensorDataset(torch.from_numpy(test_stimulus))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_experiment(\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        group: str = None,\n",
    "        max_iterations: int = 10001,\n",
    "        notes: str = None,\n",
    "):\n",
    "    config = {}\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    sample = train_dataset[0]\n",
    "    betas_shape = sample['betas'].shape\n",
    "    stimulus_shape = sample['stimulus'].shape\n",
    "    \n",
    "    model_params = dict(\n",
    "        layer_sizes=[\n",
    "            betas_shape[0],\n",
    "            512,\n",
    "            stimulus_shape[0],\n",
    "        ],\n",
    "        dropout_p=0.9,\n",
    "    )\n",
    "    model = DenseNetwork(**model_params)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion_params = dict()\n",
    "    criterion = CosineSimilarityLoss(**criterion_params)\n",
    "    \n",
    "    optimizer_params = dict(lr=1e-4)\n",
    "    optimizer = Adam(\n",
    "        params=model.parameters(),\n",
    "        **optimizer_params,\n",
    "    )\n",
    "    \n",
    "    training_params = dict(\n",
    "        batch_size=128,\n",
    "        evaluation_interval=250,\n",
    "        evaluation_subset_size=500,\n",
    "    )\n",
    "    experiment = TorchExperiment(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        device=device,\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        mode='decode',\n",
    "        **training_params\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        **config,\n",
    "        'model': model,\n",
    "        **model_params,\n",
    "        'criterion': criterion,\n",
    "        **criterion_params,\n",
    "        'optimizer': optimizer,\n",
    "        **optimizer_params,\n",
    "        **training_params,\n",
    "    }\n",
    "    wandb.init(project='neuro-ml', config=config, group=group, notes=notes)\n",
    "    wandb.define_metric(\"*\", summary=\"max\")\n",
    "    wandb.define_metric(\"*\", summary=\"min\")\n",
    "\n",
    "    experiment.train_model(max_iterations=max_iterations, logger=wandb.log)\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mefirdc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>G:\\Github Repositories\\Google Drive\\Repositories\\neuro-ml\\wandb\\run-20220510_025317-xkf2mnze</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/efirdc/neuro-ml/runs/xkf2mnze\" target=\"_blank\">denim-plant-3</a></strong> to <a href=\"https://wandb.ai/efirdc/neuro-ml\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10001/10001 [00:51<00:00, 192.48it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment = decoding_experiment(\n",
    "    train_dataset, \n",
    "    test_dataset,\n",
    ")\n",
    "\n",
    "_, stimulus_prediction = experiment.run_all(test_dataset)\n",
    "\n",
    "save_file_path = Path('./example_results/decoding') / wandb.run.name \n",
    "save_file_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "attributes = dict(wandb.config)\n",
    "attributes['wandb_run_name'] = wandb.run.name\n",
    "attributes['wandb_run_url'] = wandb.run.url\n",
    "attributes['wandb_group'] = wandb.run.group\n",
    "attributes['wandb_notes'] = wandb.run.notes\n",
    "\n",
    "with h5py.File(save_file_path / 'results.hdf5', 'a') as f:\n",
    "\n",
    "    for k, v in attributes.items():\n",
    "        f.attrs[k] = v\n",
    "    f.attrs['iteration'] = experiment.iteration\n",
    "    require_dataset(f, 'test/stimulus_pred', stimulus_prediction.detach().cpu())\n",
    "\n",
    "    model_group = f.require_group('model')\n",
    "    for param_name, weights in experiment.model.state_dict().items():\n",
    "        weights = weights.cpu()\n",
    "        require_dataset(model_group, param_name, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_experiment(\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        group: str = None,\n",
    "        max_iterations: int = 10001,\n",
    "        notes: str = None,\n",
    "):\n",
    "    config = {}\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    sample = train_dataset[0]\n",
    "    betas_shape = sample['betas'].shape\n",
    "    stimulus_shape = sample['stimulus'].shape\n",
    "    \n",
    "    model_params = dict(\n",
    "        layer_sizes=[\n",
    "            stimulus_shape[0],\n",
    "            betas_shape[0],\n",
    "        ],\n",
    "    )\n",
    "    model = DenseNetwork(**model_params)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion_params = dict()\n",
    "    criterion = nn.MSELoss(**criterion_params)\n",
    "    \n",
    "    optimizer_params = dict(lr=1e-3)\n",
    "    optimizer = Adam(\n",
    "        params=model.parameters(),\n",
    "        **optimizer_params,\n",
    "    )\n",
    "    \n",
    "    training_params = dict(\n",
    "        batch_size=128,\n",
    "        evaluation_interval=250,\n",
    "        evaluation_subset_size=500,\n",
    "    )\n",
    "    experiment = TorchExperiment(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        device=device,\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        mode='encode',\n",
    "        **training_params\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        **config,\n",
    "        'model': model,\n",
    "        **model_params,\n",
    "        'criterion': criterion,\n",
    "        **criterion_params,\n",
    "        'optimizer': optimizer,\n",
    "        **optimizer_params,\n",
    "        **training_params,\n",
    "    }\n",
    "    wandb.init(project='neuro-ml', config=config, group=group, notes=notes)\n",
    "    wandb.define_metric(\"*\", summary=\"max\")\n",
    "    wandb.define_metric(\"*\", summary=\"min\")\n",
    "\n",
    "    experiment.train_model(max_iterations=max_iterations, logger=wandb.log)\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j717snii) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sunny-dawn-4</strong>: <a href=\"https://wandb.ai/efirdc/neuro-ml/runs/j717snii\" target=\"_blank\">https://wandb.ai/efirdc/neuro-ml/runs/j717snii</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220510_030324-j717snii\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j717snii). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>G:\\Github Repositories\\Google Drive\\Repositories\\neuro-ml\\wandb\\run-20220510_030434-kzh4bi9i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/efirdc/neuro-ml/runs/kzh4bi9i\" target=\"_blank\">pleasant-elevator-5</a></strong> to <a href=\"https://wandb.ai/efirdc/neuro-ml\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10001/10001 [00:41<00:00, 240.05it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment = encoding_experiment(\n",
    "    train_dataset, \n",
    "    test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    betas, betas_pred = experiment.run_all(test_dataset)\n",
    "\n",
    "betas_r2 = r2_score(betas, betas_pred, reduction=None)\n",
    "betas_r2_volume = reconstruct_volume(betas_r2, tuple(volume_shape), torch.from_numpy(betas_indices).long())\n",
    "\n",
    "save_file_path = Path('./example_results/encoding') / wandb.run.name \n",
    "save_file_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "attributes = dict(wandb.config)\n",
    "attributes['wandb_run_name'] = wandb.run.name\n",
    "attributes['wandb_run_url'] = wandb.run.url\n",
    "attributes['wandb_group'] = wandb.run.group\n",
    "attributes['wandb_notes'] = wandb.run.notes\n",
    "\n",
    "with h5py.File(save_file_path / f'results.hdf5', 'a') as f:\n",
    "\n",
    "    for k, v in attributes.items():\n",
    "        f.attrs[k] = v\n",
    "    f.attrs['iteration'] = experiment.iteration\n",
    "    require_dataset(f, 'test/betas_pred', betas_pred.detach().cpu())\n",
    "    require_dataset(f, 'test/betas_r2', betas_r2.detach().cpu())\n",
    "\n",
    "    model_group = f.require_group('model')\n",
    "    for param_name, weights in experiment.model.state_dict().items():\n",
    "        weights = weights.cpu()\n",
    "        require_dataset(model_group, param_name, weights)\n",
    "        \n",
    "image = nib.Nifti1Image(betas_r2_volume.T.numpy(), np.eye(4))\n",
    "nib.save(image, save_file_path / 'betas_r2.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-ml",
   "language": "python",
   "name": "neuro-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
